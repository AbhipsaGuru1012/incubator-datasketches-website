%src/bernoulliEquations.tex
%
\section{Hypothetical Sketch Produced by Bernoulli Sampling} % $(S, \theta)$
\subsection{Fixed Theta Sampling} % $\theta$
Suppose we have a stream $A$ of $n$ items $a_1, a_2, \dots, a_n$ and an arbitrary, 
fixed sampling probability $1 > \theta > 0$.
\begin{align*}
\shortintertext{The traditional Bernoulli variable, $b_i$, is defined as a random, independent, 
weighted coin flip for each item $a_i$:}
b_i &= \begin{cases}
                 1 & \text{with probability }\theta \\
                 0 & \text{with probability }1 - \theta.
            \end{cases} \\
\shortintertext{Equivalently, we could compute a uniform hash on the interval [0,1] for each $a_i$. 
The Bernoulli variable becomes:}
b_i &= \begin{cases}
                 1 & h(a_i) < \theta \\
                 0 & h(a_i) \ge \theta.
            \end{cases} 
\shortintertext{We will use this latter definition as it more closely aligns with what we actually do.}
\end{align*}
Our sketch consists of two elements, a set $S$ of hash values $h(a_i)$ selected by the above 
Bernoulli sampling process, and a predefined value of $\theta$.  
Note that we don't actually implement this algorithm. 
It is impractical as we do not know $\theta$ upfront. 
Suspend disbelief for a few moments and pretend that we did. 
The payoff will be that the mathematics is relatively straightforward.
%
%
%
\newpage
\begin{align}
\shortintertext{From the \href{https://en.wikipedia.org/wiki/Bernoulli_distribution}{Bernoulli Distribution} 
the expected value, mean and variance are}
E[b_i=1]  &= \mu  = \theta  \notag \\
\sigma^2(b_i)     &= \theta(1-\theta) \notag \\
%
\shortintertext{A stream of $n$ \href{https://en.wikipedia.org/wiki/Bernoulli_trial}{Bernoulli Trials} 
defines a sample set $S$ of size $|S|$:}
|S|                  &=  \sum_{i \in n} b_i  \quad \text{where $|S|$ is a random variable.} \notag \\
%
\shortintertext{The expected value of $|S|$ is}
E\left[|S|\right] &= E\left[ \sum_{i \in n} b_i \right] =  \sum_{i \in n} E[b_i] = n \theta. \notag \\
%
\shortintertext{Because the samples are independent, the variance is}
\sigma^2 \left( |S| \right) &= \sum_{i \in n} \sigma^2(b_i) = n \theta (1-\theta ). \notag \\
%
\shortintertext{The estimate of $n$, is simply}
\hat{n}            &= \frac{|S|}{\theta}. \label{b_estimate}\\
%
\shortintertext{To establish unbiasedness we compute the expected value of $\hat{n}$}
E[\hat{n}]            &= E\left[ \frac{1}{\theta} \sum_{i \in n} b_i \right]= 
\frac{1}{\theta}\sum_{i \in n} E(b_i) = \frac{1}{\theta} n \theta = n. \notag \\
%
\shortintertext{To understand the error, we compute the variance of $\hat{n}$,}
\sigma^2 (\hat{n}) &= \sigma^2 \left(\frac{|S|}{\theta} \right) 
= \frac{1}{\theta^2} \sigma^2 (|S|) 
=  \frac{1}{\theta^2} (n \theta (1-\theta)) 
= \frac{n}{\theta} - n. \notag \\
\shortintertext{To compute the Relative Standard Error, we divide by $n^2$ and take the square root}
\text{RSE}(\hat{n}) &= \sqrt{\frac{\sigma^2(\hat{n})}{n^2}} = \sqrt{\frac{1}{n\theta} - \frac{1}{n}} 
= \sqrt{\frac{1}{E\left[|S|\right]} - \frac{1}{n}} < \frac{1}{\sqrt{E\left[|S|\right]}} \label{b_rse}.
\end{align}
%